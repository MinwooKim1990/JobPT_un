{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\nimport bitsandbytes as bnb\\n\\nmodel_name = \"mistralai/Mistral-7B-v0.1\" \\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    load_in_8bit=True,   \\n    device_map=\"auto\",       \\n    torch_dtype=torch.float16  \\n)\\n\\ninput_text = \"Artificial intelligence is revolutionizing\"\\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\\n\\nwith torch.no_grad():\\n    outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\\n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n    \\nprint(\"Generated text:\", generated_text)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,   \n",
    "    device_map=\"auto\",       \n",
    "    torch_dtype=torch.float16  \n",
    ")\n",
    "\n",
    "input_text = \"Artificial intelligence is revolutionizing\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "print(\"Generated text:\", generated_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def quantize_model_to_fp16(model):\\n    model_fp16 = copy.deepcopy(model)\\n    model_fp16.half()\\n    return model_fp16\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def quantize_model_to_fp16(model):\n",
    "    model_fp16 = copy.deepcopy(model)\n",
    "    model_fp16.half()\n",
    "    return model_fp16\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compression experiments...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a05922b54ee481f97a04400f7f1941f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\general2\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-7B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6daefffcb2c4fa3ae3cfd8a61d5073d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289fa54f34a6414fb3a08a57173aec8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0140c450e818489993df3469d2f692aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd074164cb514f27a8fbcf5fa3fd95c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786008da92404bcbbf2f62da50a9d706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec2cbafe25c4649b8cfdd56bfc3d3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bf9eff34cc4d51801f64fa866e97ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7222190cbc04526b2e70d3f53b2126d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8982e55d4c15456b968e0cb4b3f6109c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25da3dd1bbee4fe7a6818c57df8f440d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c822d3a0e3cb4db38c4d2a167ca5a6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26be3786477c48ff8497ef4b5a0f5091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoTokenizer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "class ModelCompression:\n",
    "    def __init__(self, model_name=\"Qwen/Qwen2.5-7B-Instruct\", num_labels=2):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def prepare_data(self, max_length=128, batch_size=16):\n",
    "        \"\"\"데이터셋 준비 (SST-2 감성분석 데이터셋 사용)\"\"\"\n",
    "        dataset = load_dataset(\"glue\", \"sst2\")\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"sentence\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "\n",
    "        tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "        tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "        tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "        tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "        train_dataset = tokenized_datasets[\"train\"].select(range(1000))\n",
    "        eval_dataset = tokenized_datasets[\"validation\"].select(range(200))\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        eval_dataloader = DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        return train_dataloader, eval_dataloader\n",
    "\n",
    "    def custom_prune_model(self, amount=0.3):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                weight = module.weight.data.clone()\n",
    "                \n",
    "                weight_abs = torch.abs(weight)\n",
    "                \n",
    "                k = int(amount * weight.numel())\n",
    "                threshold = torch.kthvalue(weight_abs.reshape(-1), k).values\n",
    "                \n",
    "                mask = (weight_abs > threshold).float()\n",
    "                \n",
    "                module.weight.data.mul_(mask)\n",
    "                \n",
    "                module.register_buffer(f'weight_mask', mask)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "    def export_to_onnx(self, onnx_model_path, max_length=128):\n",
    "        self.model.cpu().eval()\n",
    "        dummy_input = {\n",
    "            'input_ids': torch.zeros(1, max_length, dtype=torch.long),\n",
    "            'attention_mask': torch.zeros(1, max_length, dtype=torch.long)\n",
    "        }\n",
    "        torch.onnx.export(\n",
    "            self.model,\n",
    "            (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "            onnx_model_path,\n",
    "            input_names=['input_ids', 'attention_mask'],\n",
    "            output_names=['logits'],\n",
    "            dynamic_axes={\n",
    "                'input_ids': {0: 'batch_size', 1: 'sequence_length'},\n",
    "                'attention_mask': {0: 'batch_size', 1: 'sequence_length'},\n",
    "                'logits': {0: 'batch_size'}\n",
    "            },\n",
    "            opset_version=14 \n",
    "        )\n",
    "        print(f\"Model exported to {onnx_model_path}\")\n",
    "\n",
    "\n",
    "    def quantize_onnx_model(self, onnx_model_path, quantized_model_path):\n",
    "        quantize_dynamic(\n",
    "            onnx_model_path,\n",
    "            quantized_model_path,\n",
    "            weight_type=QuantType.QInt8\n",
    "        )\n",
    "        print(f\"Model quantized and saved to {quantized_model_path}\")\n",
    "\n",
    "    def evaluate_onnx_model(self, quantized_model_path, eval_dataloader, max_length=128):\n",
    "        session = ort.InferenceSession(quantized_model_path)\n",
    "\n",
    "        predictions = []\n",
    "        references = []\n",
    "\n",
    "        for batch in eval_dataloader:\n",
    "            input_ids = batch['input_ids'].cpu().numpy()\n",
    "            attention_mask = batch['attention_mask'].cpu().numpy()\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "            ort_inputs = {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "            ort_outs = session.run(None, ort_inputs)\n",
    "            logits = ort_outs[0]\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "            predictions.extend(preds)\n",
    "            references.extend(labels)\n",
    "\n",
    "        accuracy = accuracy_score(references, predictions)\n",
    "        return accuracy\n",
    "\n",
    "    def knowledge_distillation(self, train_dataloader, num_epochs=3):\n",
    "        try:\n",
    "            student_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"prajjwal1/bert-tiny\",\n",
    "                num_labels=2,\n",
    "                ignore_mismatched_sizes=True\n",
    "            ).to(self.device)\n",
    "            \n",
    "            student_model.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            student_model.classifier.bias.data.zero_()\n",
    "            \n",
    "            teacher_model = self.model.to(self.device)\n",
    "            teacher_model.eval()\n",
    "            \n",
    "            optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                student_model.train()\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "                \n",
    "                for batch in train_dataloader:\n",
    "                    batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        teacher_outputs = teacher_model(**batch).logits\n",
    "                    \n",
    "                    student_outputs = student_model(**batch).logits\n",
    "                    \n",
    "                    temperature = 2.0\n",
    "                    alpha = 0.7\n",
    "                    \n",
    "                    soft_targets = F.softmax(teacher_outputs / temperature, dim=-1)\n",
    "                    soft_prob = F.log_softmax(student_outputs / temperature, dim=-1)\n",
    "                    soft_loss = F.kl_div(\n",
    "                        soft_prob,\n",
    "                        soft_targets,\n",
    "                        reduction='batchmean'\n",
    "                    ) * (temperature ** 2)\n",
    "                    \n",
    "                    hard_loss = F.cross_entropy(student_outputs, batch['labels'])\n",
    "                    loss = (alpha * hard_loss) + ((1.0 - alpha) * soft_loss)\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "                \n",
    "                avg_loss = total_loss / num_batches\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            return student_model.eval() \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Distillation 중 에러 발생: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def train_model(self, train_dataloader, num_epochs=3):\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch in train_dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluate_model(self, model, eval_dataloader):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        references = []\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in eval_dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                predictions.extend(outputs.logits.argmax(-1).cpu().numpy())\n",
    "                references.extend(batch[\"labels\"].cpu().numpy())\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        accuracy = accuracy_score(references, predictions)\n",
    "        avg_loss = total_loss / len(eval_dataloader)\n",
    "        return accuracy, avg_loss\n",
    "\n",
    "print(\"Starting compression experiments...\")\n",
    "\n",
    "compressor = ModelCompression()\n",
    "train_dataloader, eval_dataloader = compressor.prepare_data()\n",
    "\n",
    "student_accuracy = None\n",
    "student_size = None\n",
    "quantized_size = None\n",
    "\n",
    "print(\"\\n1. Training and evaluating base model...\")\n",
    "compressor.train_model(train_dataloader)\n",
    "base_accuracy, base_loss = compressor.evaluate_model(compressor.model, eval_dataloader)\n",
    "print(f\"Base Model - Accuracy: {base_accuracy:.4f}, Loss: {base_loss:.4f}\")\n",
    "base_size = sum(p.numel() * p.element_size() for p in compressor.model.parameters()) / (1024 * 1024)\n",
    "print(f\"Base Model Size: {base_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n2. Pruning model...\")\n",
    "pruned_model = compressor.custom_prune_model(amount=0.3)\n",
    "pruned_accuracy, pruned_loss = compressor.evaluate_model(pruned_model, eval_dataloader)\n",
    "print(f\"Pruned Model - Accuracy: {pruned_accuracy:.4f}, Loss: {pruned_loss:.4f}\")\n",
    "\n",
    "print(\"\\n3. Quantizing model using ONNX...\")\n",
    "onnx_model_path = \"model.onnx\"\n",
    "quantized_model_path = \"model_quantized.onnx\"\n",
    "compressor.export_to_onnx(onnx_model_path)\n",
    "compressor.quantize_onnx_model(onnx_model_path, quantized_model_path)\n",
    "quantized_accuracy = compressor.evaluate_onnx_model(quantized_model_path, eval_dataloader)\n",
    "print(f\"Quantized Model - Accuracy: {quantized_accuracy:.4f}\")\n",
    "quantized_size = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
    "print(f\"Quantized Model Size: {quantized_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n4. Training student model with distillation...\")\n",
    "student_model = compressor.knowledge_distillation(train_dataloader)\n",
    "if student_model is not None:\n",
    "    student_accuracy, student_loss = compressor.evaluate_model(student_model, eval_dataloader)\n",
    "    student_size = sum(p.numel() * p.element_size() for p in student_model.parameters()) / (1024 * 1024)\n",
    "    print(f\"Student Model - Accuracy: {student_accuracy:.4f}, Loss: {student_loss:.4f}\")\n",
    "    print(f\"Student Model Size: {student_size:.2f} MB\")\n",
    "else:\n",
    "    print(\"Student Model  - Training failed\")\n",
    "\n",
    "print(\"\\nCompression Results Summary:\")\n",
    "print(f\"Base Model     - Size: {base_size:.2f} MB, Accuracy: {base_accuracy:.4f}\")\n",
    "print(f\"Pruned Model   - Size: {base_size:.2f} MB (30% weights removed), Accuracy: {pruned_accuracy:.4f}\")\n",
    "print(f\"Quantized Model - Size: {quantized_size:.2f} MB, Accuracy: {quantized_accuracy:.4f}\")\n",
    "if student_accuracy is not None and student_size is not None:\n",
    "    print(f\"Student Model  - Size: {student_size:.2f} MB, Accuracy: {student_accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"Student Model  - Training failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
